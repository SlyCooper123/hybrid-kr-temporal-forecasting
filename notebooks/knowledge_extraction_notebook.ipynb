{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9487ae4f",
   "metadata": {},
   "source": [
    "# Knowledge Extraction Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6373211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: c:\\Users\\acer\\Desktop\\fnn-pso\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def ensure(pkg, conda=False):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except Exception:\n",
    "        if conda:\n",
    "            try:\n",
    "                subprocess.check_call([\"conda\",\"install\",\"-c\",\"conda-forge\",pkg,\"-y\"])\n",
    "            except Exception:\n",
    "                subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",pkg])\n",
    "        else:\n",
    "            subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",pkg])\n",
    "ensure('pandas'); ensure('numpy'); ensure('matplotlib'); ensure('statsmodels'); ensure('scikit_learn')\n",
    "try:\n",
    "    import pmdarima\n",
    "except Exception:\n",
    "    try: subprocess.check_call([\"conda\",\"install\",\"-c\",\"conda-forge\",\"pmdarima\",\"-y\"])\n",
    "    except Exception: subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"pmdarima\"])\n",
    "try:\n",
    "    import pygam\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"pygam\"]) \n",
    "try:\n",
    "    import ruptures\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"ruptures\"]) \n",
    "print('Python exe:', sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54d503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>consumo_gwh</th>\n",
       "      <th>tmean_c</th>\n",
       "      <th>tmax_c</th>\n",
       "      <th>tmin_c</th>\n",
       "      <th>hdd18</th>\n",
       "      <th>cdd22</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>rad_solar</th>\n",
       "      <th>humidade_relativa</th>\n",
       "      <th>nebulosidade_media</th>\n",
       "      <th>sunshine_sec</th>\n",
       "      <th>sunshine_h</th>\n",
       "      <th>wind_speed_max</th>\n",
       "      <th>wind_gusts_max</th>\n",
       "      <th>day_length_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>119.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>14.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.68</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>30286.11</td>\n",
       "      <td>8.412808</td>\n",
       "      <td>11.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>9.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>119.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.04</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>30701.15</td>\n",
       "      <td>8.528097</td>\n",
       "      <td>9.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>9.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>119.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "      <td>29536.07</td>\n",
       "      <td>8.204464</td>\n",
       "      <td>11.6</td>\n",
       "      <td>22.3</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.87</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>29503.68</td>\n",
       "      <td>8.195467</td>\n",
       "      <td>10.5</td>\n",
       "      <td>23.8</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>14.2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.87</td>\n",
       "      <td>81</td>\n",
       "      <td>22</td>\n",
       "      <td>29536.07</td>\n",
       "      <td>8.204464</td>\n",
       "      <td>10.5</td>\n",
       "      <td>23.8</td>\n",
       "      <td>9.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  consumo_gwh  tmean_c  tmax_c  tmin_c  hdd18  cdd22  precip_mm  \\\n",
       "0 2015-01-01        119.0      9.1    14.7     4.9    8.9    0.0        0.0   \n",
       "1 2015-01-01        119.0      8.5    13.3     4.7    9.5    0.0        0.0   \n",
       "2 2015-01-01        119.0      5.1    14.8    -0.6   12.9    0.0        0.0   \n",
       "3 2015-01-01        119.0      4.5    14.0    -1.0   13.5    0.0        0.0   \n",
       "4 2015-01-01        119.0      4.6    14.2    -1.1   13.4    0.0        0.0   \n",
       "\n",
       "   rad_solar  humidade_relativa  nebulosidade_media  sunshine_sec  sunshine_h  \\\n",
       "0       9.68                 79                   0      30286.11    8.412808   \n",
       "1      10.04                 77                   0      30701.15    8.528097   \n",
       "2       8.50                 81                  39      29536.07    8.204464   \n",
       "3       8.87                 82                  22      29503.68    8.195467   \n",
       "4       8.87                 81                  22      29536.07    8.204464   \n",
       "\n",
       "   wind_speed_max  wind_gusts_max  day_length_hours  \n",
       "0            11.9            19.8              9.43  \n",
       "1             9.8            16.9              9.53  \n",
       "2            11.6            22.3              9.25  \n",
       "3            10.5            23.8              9.25  \n",
       "4            10.5            23.8              9.27  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "CSV_PATH = \"dataset_meteo_com_consumo.csv\" \n",
    "OUT_DIR = Path(\"outputs_knowledge\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip().lower().replace(' ','_') for c in df.columns]\n",
    "date_col = 'date' if 'date' in df.columns else ('data' if 'data' in df.columns else None)\n",
    "assert date_col is not None, 'Coluna de data não encontrada.'\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True)\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "TARGET='consumo_gwh'\n",
    "assert TARGET in df.columns, f\"{TARGET} não encontrada\"\n",
    "meteo_candidates=['tmean_c','tmax_c','tmin_c','hdd18','cdd22','precip_mm','rad_solar','humidade_relativa','nebulosidade_media','sunshine_sec','sunshine_h','wind_speed_max','wind_gusts_max','day_length_h','day_length_hours']\n",
    "if 'sunshine_sec' in df.columns and 'sunshine_h' not in df.columns:\n",
    "    df['sunshine_h']=pd.to_numeric(df['sunshine_sec'],errors='coerce')/3600\n",
    "meteo_cols=[c for c in meteo_candidates if c in df.columns]\n",
    "pd.DataFrame({'column':df.columns,'dtype':[str(df[c].dtype) for c in df.columns]}).to_csv(OUT_DIR/'schema_columns.csv',index=False)\n",
    "df[[date_col,TARGET]+meteo_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bebea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77393"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_lags(frame, cols, lags=(1,2,3,7)):\n",
    "    f=frame.copy()\n",
    "    for c in cols:\n",
    "        if c in f.columns:\n",
    "            for L in lags:\n",
    "                f[f\"{c}_lag{L}\"]=f[c].shift(L)\n",
    "    return f\n",
    "if 'hdd18' not in df.columns and 'tmean_c' in df.columns:\n",
    "    df['hdd18']=(18-df['tmean_c']).clip(lower=0)\n",
    "if 'cdd22' not in df.columns and 'tmean_c' in df.columns:\n",
    "    df['cdd22']=(df['tmean_c']-22).clip(lower=0)\n",
    "exo_base=[c for c in ['tmean_c','hdd18','cdd22','rad_solar','humidade_relativa'] if c in df.columns]\n",
    "df=add_lags(df, exo_base, lags=(1,2,3,7))\n",
    "lag_cols=[c for c in df.columns if any(c.startswith(b+'_lag') for b in exo_base)]\n",
    "work=df[['date',TARGET]+lag_cols].dropna().copy() if 'date' in df.columns else df[['data',TARGET]+lag_cols].dropna().copy()\n",
    "work.to_csv(OUT_DIR/'model_table_lags.csv', index=False)\n",
    "len(work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3ffeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     SARIMAX Results                                     \n",
      "=========================================================================================\n",
      "Dep. Variable:                                 y   No. Observations:                77393\n",
      "Model:             SARIMAX(1, 1, 1)x(1, 0, 1, 7)   Log Likelihood             -188211.118\n",
      "Date:                           Mon, 03 Nov 2025   AIC                         376472.236\n",
      "Time:                                   11:55:14   BIC                         376703.649\n",
      "Sample:                                        0   HQIC                        376543.283\n",
      "                                         - 77393                                         \n",
      "Covariance Type:                             opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0015      0.017     -0.090      0.928      -0.034       0.031\n",
      "x2            -0.0143      0.020     -0.729      0.466      -0.053       0.024\n",
      "x3            -0.0077      0.017     -0.450      0.653      -0.041       0.026\n",
      "x4             0.0196      0.013      1.525      0.127      -0.006       0.045\n",
      "x5            -0.0089      0.018     -0.510      0.610      -0.043       0.025\n",
      "x6            -0.0147      0.021     -0.713      0.476      -0.055       0.026\n",
      "x7            -0.0030      0.019     -0.163      0.870      -0.039       0.033\n",
      "x8             0.0101      0.014      0.736      0.462      -0.017       0.037\n",
      "x9             0.0065      0.023      0.276      0.783      -0.040       0.053\n",
      "x10            0.0185      0.028      0.671      0.502      -0.035       0.072\n",
      "x11           -0.0125      0.025     -0.494      0.621      -0.062       0.037\n",
      "x12           -0.0123      0.021     -0.597      0.551      -0.053       0.028\n",
      "x13           -0.0042      0.005     -0.775      0.439      -0.015       0.006\n",
      "x14           -0.0002      0.006     -0.026      0.979      -0.012       0.011\n",
      "x15           -0.0021      0.005     -0.386      0.700      -0.013       0.009\n",
      "x16           -0.0032      0.004     -0.835      0.404      -0.011       0.004\n",
      "x17            0.0012      0.002      0.638      0.524      -0.003       0.005\n",
      "x18            0.0013      0.002      0.592      0.554      -0.003       0.005\n",
      "x19            0.0002      0.002      0.103      0.918      -0.004       0.004\n",
      "x20            0.0011      0.002      0.717      0.473      -0.002       0.004\n",
      "ar.L1         -0.2837     19.574     -0.014      0.988     -38.648      38.081\n",
      "ma.L1          0.2838     19.573      0.015      0.988     -38.079      38.646\n",
      "ar.S.L7       -0.3310      1.386     -0.239      0.811      -3.047       2.385\n",
      "ma.S.L7        0.3310      1.379      0.240      0.810      -2.372       3.034\n",
      "sigma2         7.5871      0.005   1526.118      0.000       7.577       7.597\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):          49659577.32\n",
      "Prob(Q):                              1.00   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.16   Skew:                             0.22\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                       127.10\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "  metric     value\n",
      "0   RMSE  2.788946\n",
      "1    MAE  0.347718\n",
      "2     R²  0.965530\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = work[TARGET].astype(float).values\n",
    "X = work[lag_cols].astype(float).values if lag_cols else None\n",
    "order=(1,1,1); seasonal_order=(1,0,1,7)\n",
    "\n",
    "res = SARIMAX(\n",
    "    y, exog=X, order=order, seasonal_order=seasonal_order,\n",
    "    enforce_stationarity=False, enforce_invertibility=False\n",
    ").fit(disp=False)\n",
    "\n",
    "print(res.summary())\n",
    "\n",
    "\n",
    "param_names = np.array(res.param_names, dtype=object)  \n",
    "coef = np.array(res.params, dtype=float)               \n",
    "pvals = np.array(res.pvalues, dtype=float)             \n",
    "\n",
    "params = pd.DataFrame({\"coef\": coef, \"pvalue\": pvals}, index=param_names)\n",
    "params.index.name = \"param\"\n",
    "params.to_csv(OUT_DIR / 'arimax_params.csv')\n",
    "\n",
    "\n",
    "pred = res.get_prediction() \n",
    "sf = pred.summary_frame()    \n",
    "\n",
    "\n",
    "date_col = 'date' if 'date' in work.columns else ('data' if 'data' in work.columns else None)\n",
    "if date_col is None:\n",
    "    \n",
    "    dates = pd.RangeIndex(start=0, stop=len(y), step=1)\n",
    "else:\n",
    "    dates = pd.to_datetime(work[date_col])\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'y': y,\n",
    "    'yhat': sf['mean'].to_numpy(),\n",
    "    'lo': sf.get('mean_ci_lower', sf.iloc[:, 0]).to_numpy(),\n",
    "    'hi': sf.get('mean_ci_upper', sf.iloc[:, 1]).to_numpy(),\n",
    "})\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(pred_df['y'], pred_df['yhat']))\n",
    "mae = mean_absolute_error(pred_df['y'], pred_df['yhat'])\n",
    "r2  = r2_score(pred_df['y'], pred_df['yhat'])\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'metric': ['RMSE','MAE','R²'],\n",
    "    'value': [rmse, mae, r2]\n",
    "})\n",
    "metrics.to_csv(OUT_DIR / 'arimax_metrics.csv', index=False)\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(pred_df['date'], pred_df['y'], label='obs')\n",
    "plt.plot(pred_df['date'], pred_df['yhat'], label='fit')\n",
    "plt.fill_between(pred_df['date'], pred_df['lo'], pred_df['hi'], alpha=0.2, label='95% CI')\n",
    "plt.legend()\n",
    "plt.title('ARIMAX in-sample')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / 'arimax_fit_ci.png', dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e465e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAM treinado com sucesso.\n",
      "Arquivos salvos em: C:\\Users\\acer\\Desktop\\fnn-pso\\Mestrado Pedro\\outputs_knowledge\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from pygam import LinearGAM, s, l  \n",
    "\n",
    "\n",
    "TARGET = 'consumo_gwh' \n",
    "\n",
    "\n",
    "\n",
    "tcol = 'date' if 'date' in df.columns else 'data'\n",
    "gam_cols = [c for c in ['tmean_c','hdd18','cdd22','rad_solar','humidade_relativa'] if c in df.columns]\n",
    "\n",
    "gdf = df[[tcol, TARGET] + gam_cols].copy()\n",
    "\n",
    "gdf[tcol] = pd.to_datetime(gdf[tcol], errors='coerce')\n",
    "gdf = gdf.dropna(subset=[tcol, TARGET])\n",
    "\n",
    "\n",
    "gdf['doy'] = gdf[tcol].dt.dayofyear.astype(float)\n",
    "gdf['doy01'] = (gdf['doy'] - 1) / 365.25  # ~[0,1)\n",
    "\n",
    "H = 3  \n",
    "for h in range(1, H+1):\n",
    "    gdf[f'sin{h}'] = np.sin(2 * np.pi * h * gdf['doy01'])\n",
    "    gdf[f'cos{h}'] = np.cos(2 * np.pi * h * gdf['doy01'])\n",
    "\n",
    "fourier_cols = [f'sin{h}' for h in range(1, H+1)] + [f'cos{h}' for h in range(1, H+1)]\n",
    "\n",
    "\n",
    "feat_cols = gam_cols + fourier_cols\n",
    "gdf = gdf.dropna(subset=feat_cols) \n",
    "X_gam = gdf[feat_cols].astype(float).values\n",
    "y_gam = gdf[TARGET].astype(float).values\n",
    "\n",
    "\n",
    "terms = [s(i) for i in range(len(gam_cols))] + \\\n",
    "        [l(i) for i in range(len(gam_cols), len(feat_cols))]\n",
    "\n",
    "\n",
    "if not terms:\n",
    "    raise ValueError(\"Nenhum termo foi definido. Verifique se há colunas em gam_cols ou defina H>0.\")\n",
    "term_list = reduce(lambda a, b: a + b, terms)\n",
    "\n",
    "\n",
    "assert X_gam.shape[1] == len(terms), f\"Nº colunas X ({X_gam.shape[1]}) != nº termos ({len(terms)})\"\n",
    "if not (np.isfinite(X_gam).all() and np.isfinite(y_gam).all()):\n",
    "    raise ValueError(\"Existem NaN/Inf em X_gam ou y_gam. Limpe os dados antes do ajuste.\")\n",
    "\n",
    "\n",
    "gam = LinearGAM(term_list).fit(X_gam, y_gam)\n",
    "\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame([{\n",
    "    'pseudo_r2': gam.statistics_['pseudo_r2'],\n",
    "    'edof': gam.statistics_['edof'],\n",
    "    'gcv': gam.statistics_['GCV']\n",
    "}]).to_csv(OUT_DIR / 'gam_summary.csv', index=False)\n",
    "\n",
    "\n",
    "for i, name in enumerate(feat_cols):\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
    "    plt.title(f'Shape: {name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / f'gam_shape_{name}.png', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "yhat_gam = gam.predict(X_gam)\n",
    "gam_pred_df = pd.DataFrame({\n",
    "    'date': gdf[tcol].values,\n",
    "    'y': y_gam,\n",
    "    'yhat': yhat_gam,\n",
    "    'resid': y_gam - yhat_gam\n",
    "})\n",
    "gam_pred_df.to_csv(OUT_DIR / 'gam_in_sample_pred.csv', index=False)\n",
    "\n",
    "print(\"sucess.\")\n",
    "print(f\"Saved at: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3daaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Regras salvas em: outputs_knowledge\\rulefit_rules.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_csv_robust(path):\n",
    "    trials = [(e,s) for e in (\"utf-8\",\"latin1\") for s in (\",\",\";\",\"\\t\")]\n",
    "    last=None\n",
    "    for enc,sep in trials:\n",
    "        try:\n",
    "            df=pd.read_csv(path,encoding=enc,sep=sep)\n",
    "            if df.shape[1]>=6 and df.shape[0]>=100:\n",
    "                print(f\"[info] Parsed CSV enc={enc} sep='{sep}' shape={df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise RuntimeError(f\"Could not parse CSV {path}. Last error: {last}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    CSV_PATH = \"dataset_meteo_com_consumo.csv\"  \n",
    "    df = _read_csv_robust(CSV_PATH)\n",
    "\n",
    "\n",
    "df.columns = [c.strip().lower().replace(\" \",\"_\") for c in df.columns]\n",
    "\n",
    "\n",
    "TARGET = None\n",
    "for cand in [\"consumo_gwh\",\"consumo\",\"target\",\"gwh\",\"consumo_diario_gwh\"]:\n",
    "    if cand in df.columns:\n",
    "        TARGET = cand\n",
    "        break\n",
    "assert TARGET is not None, \"Dont found(TARGET).\"\n",
    "\n",
    "\n",
    "import re\n",
    "def compute_support(rule_series: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Percentage of lines that satisfy each rule (rules with ‘&’ and simple comparators).\"\"\"\n",
    "    sup = []\n",
    "    for rule in rule_series.astype(str):\n",
    "        mask = np.ones(len(X), dtype=bool)\n",
    "        for part in re.split(r\"\\s*&\\s*\", rule):\n",
    "            m = re.match(r\"\\s*([\\w_]+)\\s*(<=|>=|<|>)\\s*([-+]?\\d+(\\.\\d+)?)\\s*\", part)\n",
    "            if m:\n",
    "                feat, op, thr = m.group(1), m.group(2), float(m.group(3))\n",
    "                if feat not in X.columns:\n",
    "                    mask &= False\n",
    "                else:\n",
    "                    s = pd.to_numeric(X[feat], errors=\"coerce\")\n",
    "                    if op == \"<\":   mask &= (s < thr)\n",
    "                    elif op == \"<=\": mask &= (s <= thr)\n",
    "                    elif op == \">\":  mask &= (s > thr)\n",
    "                    elif op == \">=\": mask &= (s >= thr)\n",
    "            else:\n",
    "                m2 = re.match(r\"\\s*([\\w_]+)\\s*==\\s*([01])\\s*\", part)\n",
    "                if m2:\n",
    "                    feat, val = m2.group(1), int(m2.group(2))\n",
    "                    if feat not in X.columns:\n",
    "                        mask &= False\n",
    "                    else:\n",
    "                        mask &= (pd.to_numeric(X[feat], errors=\"coerce\") == val)\n",
    "        sup.append(mask.mean())\n",
    "    return pd.Series(sup)\n",
    "\n",
    "\n",
    "rules_out = Path(\"outputs_knowledge\") / \"rulefit_rules.csv\"\n",
    "rules_out.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ok = False; impl = None\n",
    "try:\n",
    "    from rulefit import RuleFit            \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    ok = True; impl = \"rulefit\"\n",
    "except Exception:\n",
    "    try:\n",
    "        from imodels import RuleFitRegressor as RuleFit   \n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        ok = True; impl = \"imodels\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if ok:\n",
    " \n",
    "    rcols = [c for c in ['tmean_c','hdd18','cdd22','rad_solar','humidade_relativa']\n",
    "             if c in df.columns]\n",
    "    assert rcols, \"Nenhuma feature meteo base encontrada em df.\"\n",
    "    rdf = df[[TARGET] + rcols].dropna().copy()\n",
    "    Xr  = rdf[rcols].astype(float)\n",
    "    yr  = rdf[TARGET].astype(float).values\n",
    "\n",
    "   \n",
    "    MAX_SAMPLES = 10000\n",
    "    if len(Xr) > MAX_SAMPLES:\n",
    "        idx = np.random.RandomState(0).choice(len(Xr), size=MAX_SAMPLES, replace=False)\n",
    "        Xfit = Xr.iloc[idx].copy(); yfit = yr[idx].copy()\n",
    "    else:\n",
    "        Xfit = Xr; yfit = yr\n",
    "\n",
    "   \n",
    "    tree_gen = RandomForestRegressor(\n",
    "        n_estimators=140,     \n",
    "        max_depth=4,         \n",
    "        min_samples_leaf=50, \n",
    "        n_jobs=-1,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "   \n",
    "    rf_kwargs = dict(\n",
    "        tree_generator=tree_gen,\n",
    "        tree_size=4,\n",
    "        max_rules=500,\n",
    "        sample_fract=0.5,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    rf = RuleFit(**{k:v for k,v in rf_kwargs.items()\n",
    "                    if k in RuleFit.__init__.__code__.co_varnames})\n",
    "    rf.fit(Xfit.values, yfit, feature_names=rcols)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        rules = rf.get_rules()\n",
    "        \n",
    "        coef_col = 'coef' if 'coef' in rules.columns else ('coefficient' if 'coefficient' in rules.columns else None)\n",
    "\n",
    "        \n",
    "        if 'type' in rules.columns:\n",
    "            rules = rules[rules['type'] != 'linear']\n",
    "        if coef_col is not None:\n",
    "            rules = rules[rules[coef_col] != 0]\n",
    "\n",
    "      \n",
    "        if 'support' not in rules.columns:\n",
    "            rules['support'] = compute_support(rules['rule'], Xr)\n",
    "\n",
    "        \n",
    "        rules['len'] = rules['rule'].str.count('&') + 1\n",
    "        rules = rules[(rules['len'] <= 3) & (rules['support'] >= 0.02)]\n",
    "\n",
    "        \n",
    "        if coef_col is not None:\n",
    "            rules['score'] = rules[coef_col].abs() * rules['support']\n",
    "            rules = rules.sort_values('score', ascending=False)\n",
    "\n",
    "        \n",
    "        cols_to_save = ['rule','support','len'] + ([coef_col] if coef_col else [])\n",
    "        rules[cols_to_save].to_csv(rules_out, index=False)\n",
    "        print(f\"[ok] Rules saved at: {rules_out}\")\n",
    "    except Exception as e:\n",
    "        print(\"[warn] get_rules failed, saving feature importance.\", e)\n",
    "        imp = getattr(rf, 'feature_importances_', None)\n",
    "        if imp is not None:\n",
    "            pd.DataFrame({'feature': rcols, 'importance': imp}).to_csv(rules_out, index=False)\n",
    "            print(f\"[ok] Amounts saved in: {rules_out}\")\n",
    "else:\n",
    "    rules_out.write_text(\"# RuleFit not available; install 'rulefit' or 'imodels'.\\n\", encoding=\"utf-8\")\n",
    "    print(\"[erro] Neither ‘rulefit’ nor ‘imodels’ available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e5ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Série para STL: n=1548 pontos, periodicidade=7\n",
      "[ok] STL e ruptures finalizados. bkps=190 segmentos → C:\\Users\\acer\\Desktop\\fnn-pso\\Mestrado Pedro\\outputs_knowledge\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import ruptures as rpt\n",
    "\n",
    "\n",
    "PERIOD = 7                 \n",
    "USE_WEEKLY = True          \n",
    "MAX_POINTS = 20000         \n",
    "RUPTURES_MODEL = \"l2\"      \n",
    "RUPTURES_JUMP = 7          \n",
    "RUPTURES_MIN_SIZE = 7      \n",
    "PENALTY = 10               \n",
    "N_BKPS = None             \n",
    "\n",
    "\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs_knowledge\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "tcol = \"date\" if \"date\" in df.columns else (\"data\" if \"data\" in df.columns else None)\n",
    "assert tcol is not None, \"Coluna de data não encontrada (date/data).\"\n",
    "\n",
    "tmp = df[[tcol, TARGET]].copy()\n",
    "tmp[tcol] = pd.to_datetime(tmp[tcol], errors=\"coerce\")\n",
    "tmp = tmp.dropna(subset=[tcol, TARGET])\n",
    "\n",
    "\n",
    "ts = (tmp.groupby(tmp[tcol].dt.date)[TARGET]\n",
    "         .mean()                               \n",
    "         .astype(\"float32\")\n",
    "         .sort_index())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if USE_WEEKLY and len(ts) > MAX_POINTS:\n",
    "    ts = (ts.to_frame(\"y\")\n",
    "            .set_index(pd.to_datetime(ts.index))\n",
    "            .resample(\"W\")\n",
    "            .mean()[\"y\"]\n",
    "            .astype(\"float32\"))\n",
    "    PERIOD = 52  \n",
    "\n",
    "print(f\"[info] Série para STL: n={len(ts)} pontos, periodicidade={PERIOD}\")\n",
    "\n",
    "\n",
    "stl = STL(ts, period=PERIOD, robust=True)\n",
    "res = stl.fit()\n",
    "\n",
    "\n",
    "res.trend.astype(\"float32\").to_frame(\"trend\").to_csv(OUT_DIR/\"stl_trend.csv\")\n",
    "res.seasonal.astype(\"float32\").to_frame(\"seasonal\").to_csv(OUT_DIR/\"stl_seasonal.csv\")\n",
    "res.resid.astype(\"float32\").to_frame(\"resid\").to_csv(OUT_DIR/\"stl_resid.csv\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(311); plt.plot(ts.index, ts.values); plt.title(\"Observed\")\n",
    "plt.subplot(312); plt.plot(ts.index, res.trend.values); plt.title(\"Trend\")\n",
    "plt.subplot(313); plt.plot(ts.index, res.seasonal.values); plt.title(\"Seasonal\")\n",
    "plt.tight_layout(); plt.savefig(OUT_DIR/\"stl_components.png\", dpi=150); plt.close()\n",
    "\n",
    "\n",
    "signal = np.asarray(res.resid.dropna().values, dtype=np.float32)\n",
    "\n",
    "algo = rpt.Pelt(model=RUPTURES_MODEL, min_size=RUPTURES_MIN_SIZE, jump=RUPTURES_JUMP).fit(signal)\n",
    "\n",
    "if N_BKPS is None:\n",
    "    bkps = algo.predict(pen=PENALTY)\n",
    "else:\n",
    "    bkps = algo.predict(n_bkps=N_BKPS)\n",
    "\n",
    "pd.DataFrame({\"break_index\": bkps}).to_csv(OUT_DIR/\"ruptures_breakpoints.csv\", index=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(res.trend.index, res.trend.values, label=\"Trend\", linewidth=1.3)\n",
    "for i, b in enumerate(bkps[:-1]): \n",
    "    if 0 < b < len(res.trend):\n",
    "        plt.axvline(res.trend.index[b], linestyle=\"--\", alpha=0.4)\n",
    "plt.title(\"Trend with detected change-points\")\n",
    "plt.tight_layout(); plt.savefig(OUT_DIR/\"trend_with_bkps.png\", dpi=150); plt.close()\n",
    "\n",
    "print(f\"[ok] STL and ruptures finalized. bkps={len(bkps)-1} segmentos → {OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
