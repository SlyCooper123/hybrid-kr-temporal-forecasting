{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2973598-4d20-45f9-bd78-3b11edf70138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--latex] --inputs INPUTS [INPUTS ...]\n",
      "                             [--out-root OUT_ROOT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --inputs\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcata\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script: feature_analysis_pack.py\n",
    "Purpose:\n",
    "    Run a consistent \"feature analysis\" on 1..N CSV datasets (national, zonal, consolidated),\n",
    "    focusing on data integrity, descriptive statistics, and feature relevance w.r.t. daily\n",
    "    national consumption -- without training predictive models.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LATEX_EXPORT = False\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# LaTeX helpers\n",
    "# ------------------------------\n",
    "def ensure_dir(d: Path):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def df_to_latex(df: pd.DataFrame, out_tex: Path, caption: str, label: str):\n",
    "    \"\"\"Exports a DataFrame to LaTeX (simple table, booktabs).\"\"\"\n",
    "    try:\n",
    "        ensure_dir(out_tex.parent)\n",
    "      \n",
    "        df_limited = df.copy()\n",
    "       \n",
    "        for c in df_limited.columns:\n",
    "            if pd.api.types.is_float_dtype(df_limited[c]):\n",
    "                df_limited[c] = df_limited[c].astype(float).round(4)\n",
    "        tex = df_limited.to_latex(\n",
    "            index=False,\n",
    "            escape=True,\n",
    "            longtable=False,\n",
    "            bold_rows=False,\n",
    "            caption=caption,\n",
    "            label=label,\n",
    "        )\n",
    "        header = (\n",
    "            \"%% Auto-generated by feature_analysis_pack.py -- do not edit by hand\\n\"\n",
    "        )\n",
    "        out_tex.write_text(header + tex, encoding=\"utf-8\")\n",
    "        log(f\"Saved LaTeX: {out_tex}\")\n",
    "    except Exception as e:\n",
    "        log(f\"LaTeX export failed for {out_tex.name}: {e}\")\n",
    "\n",
    "\n",
    "def export_key_tables_to_latex(out_dir: Path, ds_name: str):\n",
    "    \"\"\"Reads CSVs generated in the dataset and exports\"\"\"\n",
    "    latex_dir = out_dir / \"latex\"\n",
    "    ensure_dir(latex_dir)\n",
    "\n",
    "    def try_tex(csv_name: str, caption: str, label_suffix: str):\n",
    "        csv_path = out_dir / csv_name\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            tex_path = latex_dir / (csv_name.replace(\".csv\", \".tex\"))\n",
    "            df_to_latex(df, tex_path, caption, f\"tab:{ds_name}_{label_suffix}\")\n",
    "        else:\n",
    "            log(f\"(skip) CSV not found for LaTeX: {csv_name}\")\n",
    "\n",
    "    try_tex(\"schema_columns.csv\", f\"Schema (columns and dtypes) ‚Äî {ds_name}.\", \"schema\")\n",
    "    try_tex(\n",
    "        \"missingness_by_column.csv\",\n",
    "        f\"Missingness by column (%) ‚Äî {ds_name}.\",\n",
    "        \"missingness\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"date_coverage.csv\", f\"Date coverage (min/max) ‚Äî {ds_name}.\", \"date_coverage\"\n",
    "    )\n",
    "    try_tex(\"date_rows_per_year.csv\", f\"Rows per year ‚Äî {ds_name}.\", \"rows_year\")\n",
    "    try_tex(\n",
    "        \"date_rows_per_year_month.csv\",\n",
    "        f\"Rows per year-month ‚Äî {ds_name}.\",\n",
    "        \"rows_year_month\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"integrity_checks.csv\", f\"Integrity checks summary ‚Äî {ds_name}.\", \"integrity\"\n",
    "    )\n",
    "    try_tex(\n",
    "        \"numeric_descriptives.csv\",\n",
    "        f\"Descriptive statistics of numeric features ‚Äî {ds_name}.\",\n",
    "        \"descriptives\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"outlier_iqr_summary.csv\",\n",
    "        f\"Outlier summary by IQR rule ‚Äî {ds_name}.\",\n",
    "        \"outliers\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"correlations_pearson.csv\",\n",
    "        f\"Pearson correlation with consumption ‚Äî {ds_name}.\",\n",
    "        \"pearson\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"correlations_spearman.csv\",\n",
    "        f\"Spearman correlation with consumption ‚Äî {ds_name}.\",\n",
    "        \"spearman\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"partial_corr_month.csv\",\n",
    "        f\"Partial correlation (controlling for month) ‚Äî {ds_name}.\",\n",
    "        \"partial_corr\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"mutual_information.csv\",\n",
    "        f\"Mutual information with consumption ‚Äî {ds_name}.\",\n",
    "        \"mi\",\n",
    "    )\n",
    "    try_tex(\n",
    "        \"corr_matrix.csv\",\n",
    "        f\"Feature correlation matrix (flattened) ‚Äî {ds_name}.\",\n",
    "        \"corr_matrix\",\n",
    "    )\n",
    "    try:\n",
    "        \n",
    "        corr_path = out_dir / \"corr_matrix.csv\"\n",
    "        if corr_path.exists():\n",
    "            corr_df = pd.read_csv(corr_path)\n",
    "            if \"feature\" in corr_df.columns:\n",
    "                \n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                corr_df = pd.read_csv(corr_path, header=0)\n",
    "                \n",
    "                if corr_df.columns[0] != \"feature\":\n",
    "                    corr_df = corr_df.rename(columns={corr_df.columns[0]: \"feature\"})\n",
    "                long = corr_df.melt(\n",
    "                    id_vars=[\"feature\"], var_name=\"feature_2\", value_name=\"pearson\"\n",
    "                )\n",
    "                df_to_latex(\n",
    "                    long,\n",
    "                    (out_dir / \"latex\" / \"corr_matrix_long.tex\"),\n",
    "                    f\"Feature correlation matrix (long format) ‚Äî {ds_name}.\",\n",
    "                    f\"tab:{ds_name}_corr_matrix_long\",\n",
    "                )\n",
    "    except Exception as e:\n",
    "        log(f\"LaTeX export (corr matrix long) failed: {e}\")\n",
    "\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{now}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    trials = [(e, s) for e in (\"utf-8\", \"latin1\") for s in (\",\", \";\", \"\\t\")]\n",
    "    last_err = None\n",
    "    for enc, sep in trials:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc, sep=sep)\n",
    "            if df.shape[1] >= 6 and df.shape[0] >= 100:\n",
    "                log(f\"Parsed CSV with enc={enc} sep='{sep}' shape={df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Could not parse CSV '{path}'. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_columns(df: pd.DataFrame) -> Tuple[str, Optional[str], Optional[str]]:\n",
    "    date_col = None\n",
    "    for c in df.columns:\n",
    "        if c in (\"date\", \"data\"):\n",
    "            date_col = c\n",
    "            break\n",
    "    if date_col is None:\n",
    "        cand = [c for c in df.columns if \"date\" in c or \"data\" in c]\n",
    "        date_col = cand[0] if cand else None\n",
    "\n",
    "    zone_col = None\n",
    "    for cand in (\"zone\", \"zona\", \"municipio\", \"munic√≠pio\", \"concelho\"):\n",
    "        if cand in df.columns:\n",
    "            zone_col = cand\n",
    "            break\n",
    "\n",
    "    consumo_col = None\n",
    "    for cand in (\n",
    "        \"consumo_gwh\",\n",
    "        \"consumo\",\n",
    "        \"target\",\n",
    "        \"gwh\",\n",
    "        \"consumo_(gwh)\",\n",
    "        \"consumo_diario_gwh\",\n",
    "    ):\n",
    "        if cand in df.columns:\n",
    "            consumo_col = cand\n",
    "            break\n",
    "\n",
    "    return date_col, zone_col, consumo_col\n",
    "\n",
    "\n",
    "def coerce_dates(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\", dayfirst=True)\n",
    "    if df[date_col].isna().all():\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def derive_meteo_numeric(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    df = df.copy()\n",
    "    meteo_candidates = [\n",
    "        \"tmean_c\",\n",
    "        \"tmax_c\",\n",
    "        \"tmin_c\",\n",
    "        \"hdd18\",\n",
    "        \"cdd22\",\n",
    "        \"amp_termica\",\n",
    "        \"precip_mm\",\n",
    "        \"rad_solar\",\n",
    "        \"sunshine_sec\",\n",
    "        \"humidade_relativa\",\n",
    "        \"nebulosidade_media\",\n",
    "        \"wind_speed_max\",\n",
    "        \"wind_gusts_max\",\n",
    "        \"day_length_hours\",\n",
    "        \"sunshine_h\",\n",
    "        \"day_length_h\",\n",
    "        \"relative_humidity_2m_mean\",\n",
    "        \"cloudcover_mean\",\n",
    "    ]\n",
    "    present = [c for c in meteo_candidates if c in df.columns]\n",
    "    for c in present:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"sunshine_sec\" in df.columns and \"sunshine_h\" not in df.columns:\n",
    "        df[\"sunshine_h\"] = pd.to_numeric(df[\"sunshine_sec\"], errors=\"coerce\") / 3600.0\n",
    "        present.append(\"sunshine_h\")\n",
    "    if \"day_length_hours\" in df.columns and \"day_length_h\" not in df.columns:\n",
    "        df[\"day_length_h\"] = pd.to_numeric(df[\"day_length_hours\"], errors=\"coerce\")\n",
    "        present.append(\"day_length_h\")\n",
    "    return df, present\n",
    "\n",
    "\n",
    "def expand_with_aggregated_columns(\n",
    "    df: pd.DataFrame, meteo_cols: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"Includes typical aggregate variants from the national view (ex.: tmean_c_mean, hdd18_mean).\"\"\"\n",
    "    cols = set(meteo_cols)\n",
    "    bases = [\n",
    "        \"tmean_c\",\n",
    "        \"tmax_c\",\n",
    "        \"tmin_c\",\n",
    "        \"hdd18\",\n",
    "        \"cdd22\",\n",
    "        \"amp_termica\",\n",
    "        \"precip_mm\",\n",
    "        \"rad_solar\",\n",
    "        \"sunshine_h\",\n",
    "        \"humidade_relativa\",\n",
    "        \"nebulosidade_media\",\n",
    "        \"wind_speed_max\",\n",
    "        \"wind_gusts_max\",\n",
    "        \"day_length_h\",\n",
    "    ]\n",
    "    for b in bases:\n",
    "        cand = f\"{b}_mean\"\n",
    "        if cand in df.columns:\n",
    "            cols.add(cand)\n",
    "    for suf in [\"_min\", \"_max\", \"_std\"]:\n",
    "        cand = f\"tmean_c{suf}\"\n",
    "        if cand in df.columns:\n",
    "            cols.add(cand)\n",
    "    return list(cols)\n",
    "\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: Path) -> None:\n",
    "    df.to_csv(path, index=False)\n",
    "    log(f\"Saved: {path.resolve()} (rows={len(df):,})\")\n",
    "\n",
    "\n",
    "def plot_line(x, y, title: str, out_path: Path, xlabel: str = \"\", ylabel: str = \"\"):\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_scatter(x, y, title: str, out_path: Path, xlabel: str = \"\", ylabel: str = \"\"):\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, s=9)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_heatmap(mat: np.ndarray, labels: List[str], title: str, out_path: Path):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_bar(x, y, title: str, out_path: Path, xlabel: str = \"\", ylabel: str = \"\"):\n",
    "    plt.figure()\n",
    "    plt.bar(x, y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# INTEGRITY CHECKS\n",
    "# ---------------------------------------------------------------\n",
    "# This part ensures that data is complete and consistent:\n",
    "# - Number of rows/columns\n",
    "# - Missing values per column\n",
    "# - Temporal coverage (min/max date)\n",
    "# - Year/month record counts\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def integrity_checks(\n",
    "    df: pd.DataFrame, date_col: str, zone_col: Optional[str], out_dir: Path\n",
    ") -> None:\n",
    "    schema = pd.DataFrame(\n",
    "        {\"column\": df.columns, \"dtype\": [str(df[c].dtype) for c in df.columns]}\n",
    "    )\n",
    "    save_csv(schema, out_dir / \"schema_columns.csv\")\n",
    "\n",
    "    miss = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": df.columns,\n",
    "            \"missing_count\": [df[c].isna().sum() for c in df.columns],\n",
    "            \"missing_pct\": [100.0 * df[c].isna().mean() for c in df.columns],\n",
    "        }\n",
    "    ).sort_values(\"missing_pct\", ascending=False)\n",
    "    save_csv(miss, out_dir / \"missingness_by_column.csv\")\n",
    "\n",
    "    if df[date_col].notna().any():\n",
    "        coverage = [\n",
    "            (\"min_date\", str(df[date_col].min().date())),\n",
    "            (\"max_date\", str(df[date_col].max().date())),\n",
    "        ]\n",
    "        years = (\n",
    "            df[df[date_col].notna()]\n",
    "            .groupby(df[date_col].dt.year)\n",
    "            .size()\n",
    "            .reset_index(name=\"rows\")\n",
    "        )\n",
    "        years.columns = [\"year\", \"rows\"]\n",
    "        mask = df[date_col].notna()\n",
    "        years_ser = df.loc[mask, date_col].dt.year.rename(\"year\")\n",
    "        months_ser = df.loc[mask, date_col].dt.month.rename(\"month\")\n",
    "        months = (\n",
    "            df.loc[mask]\n",
    "            .groupby([years_ser, months_ser])\n",
    "            .size()\n",
    "            .reset_index(name=\"rows\")\n",
    "        )\n",
    "        save_csv(\n",
    "            pd.DataFrame(coverage, columns=[\"metric\", \"value\"]),\n",
    "            out_dir / \"date_coverage.csv\",\n",
    "        )\n",
    "        save_csv(years, out_dir / \"date_rows_per_year.csv\")\n",
    "        save_csv(months, out_dir / \"date_rows_per_year_month.csv\")\n",
    "\n",
    "    if zone_col:\n",
    "        dup_key = df.duplicated(subset=[date_col, zone_col]).sum()\n",
    "        dup_df = pd.DataFrame(\n",
    "            {\"metric\": [\"duplicates_date_zone\"], \"value\": [int(dup_key)]}\n",
    "        )\n",
    "        zones_day = (\n",
    "            df.groupby(date_col)[zone_col].nunique().reset_index(name=\"zones_per_day\")\n",
    "        )\n",
    "        zones_day.to_csv(out_dir / \"zones_per_day.csv\", index=False)\n",
    "        zsum = (\n",
    "            zones_day[\"zones_per_day\"]\n",
    "            .describe()\n",
    "            .to_frame(name=\"zones_per_day\")\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"stat\"})\n",
    "        )\n",
    "        zsum[\"stat\"] = \"zones_per_day_\" + zsum[\"stat\"]\n",
    "    else:\n",
    "        dup_key = df.duplicated(subset=[date_col]).sum()\n",
    "        dup_df = pd.DataFrame({\"metric\": [\"duplicates_date\"], \"value\": [int(dup_key)]})\n",
    "        zsum = pd.DataFrame(columns=[\"stat\", \"zones_per_day\"])\n",
    "\n",
    "    integrity = pd.concat([dup_df, zsum], ignore_index=True)\n",
    "    save_csv(integrity, out_dir / \"integrity_checks.csv\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# DESCRIPTIVE STATISTICS\n",
    "# ---------------------------------------------------------------\n",
    "# Computes central tendency, dispersion, skewness, kurtosis, and detects outliers via IQR.\n",
    "# üß© EXTEND HERE: Add visualization of feature distributions or boxplots for key variables.\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def feature_statistics(df: pd.DataFrame, out_dir: Path) -> List[str]:\n",
    "    # Seleciona apenas num√©ricos N√ÉO booleanos (bool causa erro em percentis)\n",
    "    numeric_cols = []\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) and not pd.api.types.is_bool_dtype(\n",
    "            df[c]\n",
    "        ):\n",
    "            numeric_cols.append(c)\n",
    "    if not numeric_cols:\n",
    "        save_csv(\n",
    "            pd.DataFrame(columns=[\"feature\"]), out_dir / \"numeric_descriptives.csv\"\n",
    "        )\n",
    "        save_csv(\n",
    "            pd.DataFrame(columns=[\"feature\", \"pct_below_fence\", \"pct_above_fence\"]),\n",
    "            out_dir / \"outlier_iqr_summary.csv\",\n",
    "        )\n",
    "        return []\n",
    "    desc = df[numeric_cols].describe().T\n",
    "    desc[\"skew\"] = df[numeric_cols].skew(numeric_only=True)\n",
    "    desc[\"kurtosis\"] = df[numeric_cols].kurtosis(numeric_only=True)\n",
    "\n",
    "    rows = []\n",
    "    for c in numeric_cols:\n",
    "        s = df[c].dropna()\n",
    "        if len(s) < 10:\n",
    "            rows.append((c, np.nan, np.nan))\n",
    "            continue\n",
    "        q1, q3 = np.percentile(s, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        pct_lo = 100.0 * (s < lo).mean()\n",
    "        pct_hi = 100.0 * (s > hi).mean()\n",
    "        rows.append((c, pct_lo, pct_hi))\n",
    "    outlier_df = pd.DataFrame(\n",
    "        rows, columns=[\"feature\", \"pct_below_fence\", \"pct_above_fence\"]\n",
    "    )\n",
    "\n",
    "    save_csv(\n",
    "        desc.reset_index().rename(columns={\"index\": \"feature\"}),\n",
    "        out_dir / \"numeric_descriptives.csv\",\n",
    "    )\n",
    "    save_csv(outlier_df, out_dir / \"outlier_iqr_summary.csv\")\n",
    "    return numeric_cols\n",
    "\n",
    "\n",
    "def partial_corr_month(\n",
    "    df: pd.DataFrame, y_col: str, x_cols: List[str], date_col: str\n",
    ") -> pd.DataFrame:\n",
    "    if y_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"partial_corr_month\"])\n",
    "    # Drop rows with missing target OR missing date (avoid NaT -> NaN month)\n",
    "    d = df.copy().dropna(subset=[y_col, date_col])\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"partial_corr_month\"])\n",
    "\n",
    "    # Extract month safely as Int64 (but no NaN remains after drop)\n",
    "    d[\"month\"] = d[date_col].dt.month.astype(\"Int64\")\n",
    "    dm = pd.get_dummies(d[\"month\"], prefix=\"m\", drop_first=True)\n",
    "\n",
    "    def residualize(target: np.ndarray, controls: np.ndarray) -> np.ndarray:\n",
    "        ones = np.ones((controls.shape[0], 1))\n",
    "        X = np.hstack([ones, controls])\n",
    "        try:\n",
    "            beta = np.linalg.pinv(X.T @ X) @ (X.T @ target)\n",
    "            resid = target - (X @ beta)\n",
    "            return resid\n",
    "        except Exception:\n",
    "            return target\n",
    "\n",
    "    y = d[y_col].astype(float).values\n",
    "    controls = dm.values\n",
    "    y_resid = residualize(y, controls)\n",
    "\n",
    "    rows = []\n",
    "    for c in x_cols:\n",
    "        if c not in d.columns:\n",
    "            continue\n",
    "        x = d[c].astype(float).values\n",
    "        x_resid = residualize(x, controls)\n",
    "        if np.isfinite(y_resid).sum() > 5 and np.isfinite(x_resid).sum() > 5:\n",
    "            r = np.corrcoef(y_resid, x_resid)[0, 1]\n",
    "        else:\n",
    "            r = np.nan\n",
    "        rows.append((c, r))\n",
    "    return pd.DataFrame(rows, columns=[\"feature\", \"partial_corr_month\"])\n",
    "\n",
    "\n",
    "def feature_relevance(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    consumo_col: Optional[str],\n",
    "    meteo_cols: List[str],\n",
    "    out_dir: Path,\n",
    ") -> None:\n",
    "    if consumo_col is None or consumo_col not in df.columns:\n",
    "        log(\"No consumption column found; skipping relevance vs consumo.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    if not meteo_cols:\n",
    "        meteo_cols = expand_with_aggregated_columns(df, meteo_cols)\n",
    "\n",
    "    if not meteo_cols:\n",
    "        log(\"No meteo features found; skipping relevance.\")\n",
    "        return\n",
    "\n",
    "    cols = [c for c in meteo_cols if c in df.columns]\n",
    "    d = df[[date_col, consumo_col] + cols].copy().dropna(subset=[consumo_col])\n",
    "\n",
    "    pear = d[[consumo_col] + cols].corr(method=\"pearson\")[consumo_col].dropna()\n",
    "    spear = d[[consumo_col] + cols].corr(method=\"spearman\")[consumo_col].dropna()\n",
    "    save_csv(\n",
    "        pear.reset_index().rename(columns={\"index\": \"feature\", consumo_col: \"pearson\"}),\n",
    "        out_dir / \"correlations_pearson.csv\",\n",
    "    )\n",
    "    save_csv(\n",
    "        spear.reset_index().rename(\n",
    "            columns={\"index\": \"feature\", consumo_col: \"spearman\"}\n",
    "        ),\n",
    "        out_dir / \"correlations_spearman.csv\",\n",
    "    )\n",
    "\n",
    "    pc = partial_corr_month(d, consumo_col, cols, date_col)\n",
    "    save_csv(pc, out_dir / \"partial_corr_month.csv\")\n",
    "\n",
    "    try:\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "        X = d[cols].astype(float).values\n",
    "        y = d[consumo_col].astype(float).values\n",
    "        if X.shape[1] > 0:\n",
    "            mi = mutual_info_regression(X, y, random_state=0)\n",
    "            mi_df = pd.DataFrame({\"feature\": cols, \"mutual_information\": mi})\n",
    "            save_csv(\n",
    "                mi_df.sort_values(\"mutual_information\", ascending=False),\n",
    "                out_dir / \"mutual_information.csv\",\n",
    "            )\n",
    "    except Exception as e:\n",
    "        log(f\"sklearn not available or MI failed ({e}); skipping mutual information.\")\n",
    "\n",
    "    # Lags: aceitar base ou base_mean\n",
    "    lag_candidates = []\n",
    "    for base in [\"tmean_c\", \"hdd18\", \"cdd22\"]:\n",
    "        if base in cols:\n",
    "            lag_candidates.append(base)\n",
    "        elif f\"{base}_mean\" in cols:\n",
    "            lag_candidates.append(f\"{base}_mean\")\n",
    "\n",
    "    rows = []\n",
    "    for c in lag_candidates:\n",
    "        s1 = d.set_index(date_col)[consumo_col].astype(float).sort_index()\n",
    "        s2 = d.set_index(date_col)[c].astype(float).sort_index()\n",
    "        for lag in range(0, 8):\n",
    "            if lag == 0:\n",
    "                r = s1.corr(s2)\n",
    "            else:\n",
    "                r = s1.corr(s2.shift(lag))\n",
    "            rows.append((c, lag, r))\n",
    "    save_csv(\n",
    "        pd.DataFrame(rows, columns=[\"feature\", \"lag_days\", \"corr\"]),\n",
    "        out_dir / \"lag_ccf_basic.csv\",\n",
    "    )\n",
    "\n",
    "    charts = out_dir / \"charts\"\n",
    "    charts.mkdir(exist_ok=True)\n",
    "    for c in lag_candidates:\n",
    "        merged = d[[date_col, consumo_col, c]].dropna().sort_values(date_col)\n",
    "        plot_scatter(\n",
    "            merged[c],\n",
    "            merged[consumo_col],\n",
    "            f\"Consumption vs {c}\",\n",
    "            charts / f\"scatter_consumo_vs_{c}.png\",\n",
    "            xlabel=c,\n",
    "            ylabel=\"consumo_gwh\",\n",
    "        )\n",
    "\n",
    "\n",
    "def collinearity_and_structure(\n",
    "    df: pd.DataFrame, meteo_cols: List[str], out_dir: Path\n",
    ") -> None:\n",
    "    charts = out_dir / \"charts\"\n",
    "    charts.mkdir(exist_ok=True)\n",
    "\n",
    "    if len(meteo_cols) >= 2:\n",
    "        corr = df[meteo_cols].corr(method=\"pearson\")\n",
    "        save_csv(\n",
    "            corr.reset_index().rename(columns={\"index\": \"feature\"}),\n",
    "            out_dir / \"corr_matrix.csv\",\n",
    "        )\n",
    "        plot_heatmap(\n",
    "            corr.values,\n",
    "            meteo_cols,\n",
    "            \"Feature Correlation Matrix\",\n",
    "            charts / \"corr_matrix_heatmap.png\",\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "\n",
    "        vif_rows = []\n",
    "        X = df[meteo_cols].dropna().astype(float)\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if X.shape[0] > 50 and X.shape[1] >= 2:\n",
    "            X = (X - X.mean()) / (X.std(ddof=0) + 1e-9)\n",
    "            for i, col in enumerate(meteo_cols):\n",
    "                y = X[col].values\n",
    "                X_ = np.delete(X.values, i, axis=1)\n",
    "                X_ = sm.add_constant(X_)\n",
    "                model = sm.OLS(y, X_).fit()\n",
    "                r2 = model.rsquared\n",
    "                vif = 1.0 / (1.0 - r2) if r2 < 0.9999 else np.inf\n",
    "                vif_rows.append((col, vif))\n",
    "            vif_df = pd.DataFrame(vif_rows, columns=[\"feature\", \"vif\"])\n",
    "            save_csv(vif_df.sort_values(\"vif\", ascending=False), out_dir / \"vif.csv\")\n",
    "    except Exception as e:\n",
    "        log(f\"statsmodels not available or VIF failed ({e}); skipping VIF.\")\n",
    "\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        X = df[meteo_cols].dropna().astype(float)\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if X.shape[0] > 100 and X.shape[1] >= 2:\n",
    "            X = (X - X.mean()) / (X.std(ddof=0) + 1e-9)\n",
    "            pca = PCA()\n",
    "            pca.fit(X)\n",
    "            explained = pd.DataFrame(\n",
    "                {\n",
    "                    \"component\": [\n",
    "                        f\"PC{i+1}\" for i in range(len(pca.explained_variance_ratio_))\n",
    "                    ],\n",
    "                    \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "                }\n",
    "            )\n",
    "            save_csv(explained, out_dir / \"pca_explained.csv\")\n",
    "    except Exception as e:\n",
    "        log(f\"sklearn not available or PCA failed ({e}); skipping PCA.\")\n",
    "\n",
    "\n",
    "def quick_timeseries(\n",
    "    df: pd.DataFrame, date_col: str, consumo_col: Optional[str], out_dir: Path\n",
    ") -> None:\n",
    "    if consumo_col is None or consumo_col not in df.columns:\n",
    "        return\n",
    "    charts = out_dir / \"charts\"\n",
    "    charts.mkdir(exist_ok=True)\n",
    "\n",
    "    d = df[[date_col, consumo_col]].dropna().sort_values(date_col)\n",
    "    if d.empty:\n",
    "        return\n",
    "\n",
    "    plot_line(\n",
    "        d[date_col],\n",
    "        d[consumo_col],\n",
    "        \"National Daily Consumption (GWh)\",\n",
    "        charts / \"consumo_timeseries.png\",\n",
    "        xlabel=\"Date\",\n",
    "        ylabel=\"GWh\",\n",
    "    )\n",
    "\n",
    "    tmp = d.copy()\n",
    "    tmp[\"month\"] = tmp[date_col].dt.month\n",
    "    by_month = tmp.groupby(\"month\")[consumo_col].mean().reset_index()\n",
    "    plot_bar(\n",
    "        by_month[\"month\"],\n",
    "        by_month[consumo_col],\n",
    "        \"Average Consumption by Month\",\n",
    "        charts / \"consumo_by_month.png\",\n",
    "        xlabel=\"Month\",\n",
    "        ylabel=\"GWh\",\n",
    "    )\n",
    "\n",
    "    tmp[\"dow\"] = tmp[date_col].dt.dayofweek\n",
    "    by_dow = tmp.groupby(\"dow\")[consumo_col].mean().reset_index()\n",
    "    plot_bar(\n",
    "        by_dow[\"dow\"],\n",
    "        by_dow[consumo_col],\n",
    "        \"Average Consumption by Day of Week (0=Mon)\",\n",
    "        charts / \"consumo_by_dow.png\",\n",
    "        xlabel=\"DOW\",\n",
    "        ylabel=\"GWh\",\n",
    "    )\n",
    "\n",
    "    series = d.set_index(date_col)[consumo_col].astype(float).sort_index()\n",
    "    x = series.values\n",
    "    x = x - np.nanmean(x)\n",
    "    max_lag = 60\n",
    "    acf = [1.0]\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        v1 = x[:-lag]\n",
    "        v2 = x[lag:]\n",
    "        ac = np.nansum(v1 * v2) / np.nansum(x * x)\n",
    "        acf.append(ac)\n",
    "    plt.figure()\n",
    "    plt.stem(range(0, max_lag + 1), acf)  # compat without 'use_line_collection'\n",
    "    plt.title(\"ACF of Daily Consumption (lag up to 60)\")\n",
    "    plt.xlabel(\"Lag\")\n",
    "    plt.ylabel(\"Autocorrelation\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(charts / \"acf_consumo_lag60.png\", dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def process_dataset(in_path: Path, out_root: Path) -> None:\n",
    "    log(f\"Loading dataset: {in_path.name}\")\n",
    "    df = read_csv_robust(in_path)\n",
    "    df = normalize_columns(df)\n",
    "    date_col, zone_col, consumo_col = detect_columns(df)\n",
    "    if date_col is None:\n",
    "        raise SystemExit(\"Date column not found (expected 'date' or 'data').\")\n",
    "    df = coerce_dates(df, date_col)\n",
    "    df, meteo_cols = derive_meteo_numeric(df)\n",
    "    meteo_cols = expand_with_aggregated_columns(df, meteo_cols)\n",
    "\n",
    "    ds_name = in_path.stem\n",
    "    out_dir = out_root / ds_name\n",
    "    charts_dir = out_dir / \"charts\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    charts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    integrity_checks(df, date_col, zone_col, out_dir)\n",
    "    numeric_cols = feature_statistics(df, out_dir)\n",
    "    feature_relevance(df, date_col, consumo_col, meteo_cols, out_dir)\n",
    "    collinearity_and_structure(df, meteo_cols, out_dir)\n",
    "    quick_timeseries(df, date_col, consumo_col, out_dir)\n",
    "\n",
    "    if LATEX_EXPORT:\n",
    "        export_key_tables_to_latex(out_dir, ds_name)\n",
    "\n",
    "    log(f\"Finished dataset: {in_path.name}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MAIN DRIVER\n",
    "# ---------------------------------------------------------------\n",
    "# Runs the full analysis for each dataset provided in arguments.\n",
    "# üß© EXTEND HERE: You can add graphical dashboards or feature importance plots here.\n",
    "# ===============================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Feature analysis pack for energy datasets (integrity, statistics, relevance).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--latex\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Export key CSV outputs as LaTeX tables (.tex)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--inputs\", nargs=\"+\", required=True, help=\"Paths to 1..N CSV files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out-root\", default=\"eda_features\", help=\"Root output directory\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    global LATEX_EXPORT\n",
    "    LATEX_EXPORT = bool(args.latex)\n",
    "\n",
    "    out_root = Path(args.out_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for inp in args.inputs:\n",
    "        process_dataset(Path(inp), out_root)\n",
    "\n",
    "    log(\"All datasets processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6c979d-42b6-4a72-9ba8-4819dd2751f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "[16:24:03] Loading dataset: final_nacional_diario.csv\n",
      "[16:24:03] Parsed CSV with enc=utf-8 sep=';' shape=(3931, 24)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\schema_columns.csv (rows=24)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\missingness_by_column.csv (rows=24)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\date_coverage.csv (rows=2)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\date_rows_per_year.csv (rows=11)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\date_rows_per_year_month.csv (rows=129)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\integrity_checks.csv (rows=1)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\numeric_descriptives.csv (rows=23)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\outlier_iqr_summary.csv (rows=23)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\correlations_pearson.csv (rows=18)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\correlations_spearman.csv (rows=18)\n",
      "[16:24:03] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\partial_corr_month.csv (rows=17)\n",
      "[16:24:05] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\mutual_information.csv (rows=17)\n",
      "[16:24:05] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\lag_ccf_basic.csv (rows=24)\n",
      "[16:24:06] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\corr_matrix.csv (rows=17)\n",
      "[16:24:07] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\vif.csv (rows=17)\n",
      "[16:24:07] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_nacional_diario\\pca_explained.csv (rows=17)\n",
      "[16:24:07] Finished dataset: final_nacional_diario.csv\n",
      "[16:24:07] Loading dataset: dataset_meteo_com_consumo.csv\n",
      "[16:24:08] Parsed CSV with enc=utf-8 sep=';' shape=(212004, 28)\n",
      "[16:24:08] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\schema_columns.csv (rows=30)\n",
      "[16:24:08] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\missingness_by_column.csv (rows=30)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\date_coverage.csv (rows=2)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\date_rows_per_year.csv (rows=11)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\date_rows_per_year_month.csv (rows=129)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\integrity_checks.csv (rows=9)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\numeric_descriptives.csv (rows=24)\n",
      "[16:24:09] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\outlier_iqr_summary.csv (rows=24)\n",
      "[16:24:10] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\correlations_pearson.csv (rows=17)\n",
      "[16:24:10] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\correlations_spearman.csv (rows=17)\n",
      "[16:24:10] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\partial_corr_month.csv (rows=16)\n",
      "[16:24:26] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\mutual_information.csv (rows=16)\n",
      "[16:24:26] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\lag_ccf_basic.csv (rows=24)\n",
      "[16:24:28] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\corr_matrix.csv (rows=16)\n",
      "[16:24:30] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\vif.csv (rows=16)\n",
      "[16:24:31] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\dataset_meteo_com_consumo\\pca_explained.csv (rows=16)\n",
      "[16:24:31] Finished dataset: dataset_meteo_com_consumo.csv\n",
      "[16:24:31] Loading dataset: final_zonal_diario.csv\n",
      "[16:24:32] Parsed CSV with enc=utf-8 sep=';' shape=(212274, 19)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\schema_columns.csv (rows=19)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\missingness_by_column.csv (rows=19)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\date_coverage.csv (rows=2)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\date_rows_per_year.csv (rows=11)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\date_rows_per_year_month.csv (rows=129)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\integrity_checks.csv (rows=9)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\numeric_descriptives.csv (rows=17)\n",
      "[16:24:32] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\outlier_iqr_summary.csv (rows=17)\n",
      "[16:24:33] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\correlations_pearson.csv (rows=17)\n",
      "[16:24:33] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\correlations_spearman.csv (rows=17)\n",
      "[16:24:33] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\partial_corr_month.csv (rows=16)\n",
      "[16:24:50] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\mutual_information.csv (rows=16)\n",
      "[16:24:51] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\lag_ccf_basic.csv (rows=24)\n",
      "[16:24:53] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\corr_matrix.csv (rows=16)\n",
      "[16:24:55] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\vif.csv (rows=16)\n",
      "[16:24:55] Saved: C:\\Users\\pcata\\00_Thesis\\eda_features\\final_zonal_diario\\pca_explained.csv (rows=16)\n",
      "[16:24:56] Finished dataset: final_zonal_diario.csv\n",
      "[16:24:56] All datasets processed.\n"
     ]
    }
   ],
   "source": [
    "!python \"C:\\Users\\pcata\\OneDrive\\Ambiente de Trabalho\\feature_analysis_pack.py\" \\\n",
    "    --inputs \\\n",
    "    \"C:\\Users\\pcata\\OneDrive\\Ambiente de Trabalho\\final_nacional_diario.csv\" \\\n",
    "    \"C:\\Users\\pcata\\OneDrive\\Ambiente de Trabalho\\dataset_meteo_com_consumo.csv\" \\\n",
    "    \"C:\\Users\\pcata\\OneDrive\\Ambiente de Trabalho\\final_zonal_diario.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72bfd4-3a93-411b-9ad8-424ee4d630c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699bfc0-940b-4714-9f1c-dc47123c4071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
